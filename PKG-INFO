Metadata-Version: 2.4
Name: qwen-code-proxy
Version: 1.0.0
Summary: OpenAI-compatible API wrapper for Qwen Code
Author: nettee
License: MIT
License-File: LICENSE
Keywords: api,cli,qwen,openai,proxy
Requires-Python: >=3.8
Requires-Dist: click<9.0,>=8.0.0
Requires-Dist: fastapi<1.0,>=0.104.0
Requires-Dist: pydantic<3.0,>=2.0.0
Requires-Dist: slowapi<1.0,>=0.1.9
Requires-Dist: uvicorn[standard]<1.0,>=0.24.0
Description-Content-Type: text/markdown

# Qwen Code Proxy

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Wrap Qwen Code as an OpenAI-compatible API service, allowing you to enjoy the free Qwen Code model through API!

- ‚úÖ **2,000 requests/day**
- ‚úÖ **60 requests/minute** rate limit
- ‚úÖ **Zero cost** for individual users

## ‚ú® Features

- üîå **OpenAI API Compatible**: Implements `/v1/chat/completions` endpoint
- üöÄ **Quick Setup**: Zero-config run with `uvx`
- ‚ö° **High Performance**: Built on FastAPI + asyncio with concurrent request support

## üì¶ Installation

1.  **Install uv**

    `uv` is an extremely fast Python package installer and resolver, written in Rust.

    ```bash
    pip install uv
    ```

2.  **Install dependencies**

    Clone this repository and run:
    
    ```bash
    uv pip install -e .
    ```

## üöÄ Quick Start

### Install Qwen Code

Follow the installation guide from [Qwen Code's official repository](https://github.com/QwenLM/qwen-code?tab=readme-ov-file#installation).

### üîë Authentication

The first time you run `qwen`, it will guide you through an authentication process using the OAuth 2.0 device flow. This is a one-time setup.

1.  **Browser-Based Login**: The application will automatically open a new tab in your web browser, directing you to the Qwen login page.
2.  **Authorization**: Log in to your Qwen account in the browser.

After successful authorization, the application will securely store the authentication tokens in `~/.qwen/oauth_creds.json`. This allows the proxy to access your Qwen account without requiring you to log in again.

### Start Qwen Code Proxy

Run the following command:
```bash
uv run qwen-code-proxy
```

Qwen Code Proxy listens on port `8765` by default. You can customize the startup port with the `--port` parameter.

After startup, test the service with curl:

```bash
curl http://localhost:8765/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer dummy-key" \
  -d '{
    "model": "qwen3-coder-plus",
    "messages": [{"role": "user", "content": "Hello! Can you introduce your self?"}]
  }'
```

### Usage Examples

#### OpenAI Client

```python
from openai import OpenAI

client = OpenAI(
    base_url='http://localhost:8765/v1',
    api_key='dummy-key'  # Any string works
)

response = client.chat.completions.create(
    model='qwen3-coder-plus',
    messages=[
        {'role': 'user', 'content': 'Hello! Can you introduce your self?'}
    ],
)

print(response.choices[0].message.content)
```

#### Kilo Code Integration

Add Model Provider in Cherry Studio settings:
- API Provider: OpenAI Compatible
- API Host: `http://localhost:8765`
- API Key: Any string works
- Model Name: `qwen3-coder-plus`
- Uncheck the "Enable Streaming"
- Uncheck the "Image Support"
- Set the "Rate limit" to "1s", because currently Qwen Code's rate limit is 60 request per minute.


## ‚öôÔ∏è Configuration Options

View command line parameters:

```bash
qwen-code-proxy --help
```

Available options:
- `--host`: Server host address (default: 127.0.0.1)
- `--port`: Server port (default: 8765)
- `--rate-limit`: Max requests per minute (default: 60)
- `--max-concurrency`: Max concurrent subprocesses (default: 4)
- `--timeout`: Qwen Code Proxy command timeout in seconds (default: 30.0)
- `--debug`: Enable debug mode (enables debug logging and file watching)

## üìÑ License

MIT License

## ü§ù Contributing

Issues and Pull Requests are welcome!

## üèóÔ∏è Origin & Attribution

This project is a fork and adaptation of [gemini-cli-proxy](https://pypi.org/project/gemini-cli-proxy), originally created by [William Liu](https://pypi.org/user/nettee3).

The original tool provided an OpenAI-compatible API layer for Gemini CLI. This version has been modified to support **Qwen Code** instead.